# Multilevel Modelling for Big Data

## ğŸ“Œ Overview  
This project explores the application of **Multilevel Models (MLM)** for analyzing very large datasets, focusing on hierarchical data structures. The study compares **Linear Mixed-Effects Models (LMM)**, **Generalized Linear Mixed-Effects Models (GLMM)**, and **Bayesian Additive Models (BAM)**, evaluating their performance, scalability, and computational efficiency.

## ğŸ› ï¸ Features  
- **Simulated Big Data**: A dataset with 2 million observations across 2,000 groups.  
- **Hierarchical Modeling**: Analysis using **LMM, GLMM, and BAM** for structured data.  
- **Performance Benchmarking**: Evaluates computation time and model efficiency.  
- **Comparison of Fixed & Random Effects** across different modeling techniques.  
- **Application on Real Data**: Uses the **Metacognition dataset** to analyze cognitive psychology experiments.  

## ğŸ“Š Methodology  
### 1ï¸âƒ£ **Data Simulation**  
- Generated a dataset with **continuous (y), binary (y_bin), and categorical (b) variables**.  
- Hierarchical structure with **random effects at the group level**.  

### 2ï¸âƒ£ **Statistical Models**  
- **Linear Mixed-Effects Model (LMM)** â†’ `lmer()` from `lme4` package  
- **Generalized Linear Mixed-Effects Model (GLMM)** â†’ `glmer()` for binary outcomes  
- **Bayesian Additive Model (BAM)** â†’ `bam()` for non-linear relationships  
- **Optimized BAM (BAM_d)** â†’ Uses `discrete=TRUE` for faster computations  

### 3ï¸âƒ£ **Performance Analysis**  
- Measured execution time (`system.time()`) for each model.  
- Compared **fixed effects, random effects, and variance components**.  
- Experimented with **thread optimization** and **parallel computing** for BAM.  

## ğŸ“¦ Dependencies  
- **R version 4.3.2**  
- `tidyverse` â€“ Data manipulation & visualization  
- `lme4` â€“ Linear & Generalized Mixed Models  
- `mgcv` â€“ Bayesian Additive Models  
- `sdamr` â€“ Additional GAM functionalities  
- `knitr` â€“ Report generation  
- `mixedup` â€“ Extracts mixed model results  

## ğŸš€ Results & Insights  
- **LMM is computationally efficient** for continuous outcomes but assumes linearity.  
- **GLMM handles binary/categorical data** but is slower due to complexity.  
- **BAM captures non-linear relationships** effectively but requires high computational power.  
- **BAM_d (discrete BAM) significantly reduces computation time**, making it scalable for large datasets.  
- **Thread optimization** beyond a certain limit (8â€“10 threads) has **diminishing returns** on performance.  

## ğŸ” Key Takeaways  
- Choose **LMM for structured continuous data** and **GLMM for categorical responses**.  
- Use **BAM for complex relationships** but optimize computation with `discrete=TRUE`.  
- **Parallel processing** helps but is **hardware-dependent**.  
- **Consider dataset size and computational power** when selecting modeling techniques.  


## ğŸ“– References  
- **Bates et al. (2015)** â€“ *Fitting Linear Mixed-Effects Models Using lme4*  
- **Wood (2017)** â€“ *Generalized Additive Models: An Introduction with R*  
- **Clark (2019)** â€“ *Mixed Models for Big Data*  
- **Speekenbrink (2023)** â€“ *Linear Mixed Models in Cognitive Science*  

## ğŸ“¬ Contact  
For questions or contributions, feel free to reach out via **GitHub Issues**.  

---
ğŸŒŸ **If you find this project helpful, consider giving it a â­ on GitHub!**  
 
