---
title: "Multilevel Models- Group 9"
author: "Md Emon Parvez"
date: "2024-03-21"
output:
  html_document: default
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
---

## Multilevel Models for Big Data: Approaches for Handling Very Large Data Sets

This notebook explores the application of multilevel models to analyze
large datasets, focusing on techniques tailored to handle very large
data sets. It delves into various approaches for modeling hierarchical
structures in data, such as linear mixed-effects models (LMM) and
generalized additive models (GAM), and discusses strategies for
efficient computation and interpretation. Through practical examples and
demonstrations, the notebook showcases methods to address the challenges
posed by big data when employing multilevel modeling techniques.

**Note**: It is important to acknowledge that minor discrepancies may exist between the R Markdown (RMD) file and its corresponding HTML output. These discrepancies primarily pertain to the presentation of output numbers, particularly those generated by functions such as `system.time()` which detail user, system, and elapsed times, as well as visualizations. Such discrepancies can arise due to inherent differences in rendering engines, display settings, or output formats between the local R environment where the RMD file is authored and the HTML rendering environment where the output is displayed. 


#### What and Why Multilevel or Mixed Model?

```{r echo=FALSE, out.width = "90%", fig.align = "center"}
knitr::include_graphics("/Users/emon/Desktop/University/Winter 2023:2024/Multilevel Modelling/image.png") 
```

Mixed Models, often denoted as MM, scrutinize the impact of independent
factors, whether categorical or numerical, on a dependent variable.
These models derive their name from their amalgamation of fixed factors,
pertinent to researchers, and random factors, whose effects are less
directly relevant. While random effects, can be accommodated, their
inclusion is typically restricted to one factor. MM serve as versatile
tools, surpassing conventional methods like analysis of variance, and
expanding upon the capabilities of linear and logistic regression
analyses. They inherently account for variability stemming from a single
factor and tolerate missing values within their framework.

**Libraries :**

-   **tidyverse:** For data manipulation and visualization.
-   **lme4:** For fitting linear mixed-effects models.
-   **mgcv:** For fitting generalized additive models.
-   **sdamr:** For additional GAM functionality.
-   **library(knitr):** For dynamic report generation.
-   **mixedup:** For extracting results from mixed models.

```{r load_libraries, echo=TRUE, message = FALSE}
# Load necessary libraries
library(tidyverse)   # For data manipulation and visualization
library(lme4)        # For fitting linear mixed-effects models
library(mgcv)        # For fitting generalized additive models
library(sdamr)       # For additional GAM functionality
library(knitr)       # For dynamic report generation
library(mixedup)     # For extracting results from mixed models
```

**Simulated Data:**

This code chunk facilitates the generation of simulated data for
illustrative purposes. Initially, the random seed is set to ensure
reproducibility. The simulated dataset comprises a substantial sample
size (N = 2,000,000), distributed across 2000 groups, each identified by
a group identifier (g) to represent the hierarchical structure. Two
variables are created: an observation-level continuous variable (x) and
a cluster-level categorical variable (b). Parameters for the standard
deviation of the random effect (sd_g) and observation (sigma) are
specified, followed by the generation of random effects (re) based on
the group structure and standard deviation. The linear predictor (lp) is
computed using the generated variables and random effects. Subsequently,
continuous (y) and binary (y_bin) target variables are generated based
on the linear predictor, with added noise. The resulting dataframe (d)
encapsulates the simulated data, facilitating the exploration and
demonstration of various modeling techniques.

**Properties:**

-   Total sample size: 2,000,000
-   Number of groups: 2000
-   Variables:
    -   x: Continuous
    -   b: Categorical
    -   y: Continuous
    -   y_bin: Binary
    -   g: Group identifier
-   Standard deviation of random effect (sd_g): 0.5
-   Standard deviation of observation (sigma): 1
-   Hierarchical structure: Groups represented by the group identifier
    (g) 
-   Target variables: Continuous (y) and binary (y_bin)
-   Purpose: Illustration of modeling techniques with hierarchical data
    structures and large datasets.

```{r simulated_data, echo=TRUE}
# Set seed for reproducibility
set.seed(12358)

# Generate simulated data
N <- 2e6             # Total sample size
n_groups <- 2000     # Number of groups
g <- rep(1:n_groups, each = N/n_groups)  # Group identifier

# Generate variables
x <- rnorm(N)                              # Observation-level continuous variable
b <- rbinom(n_groups, size = 1, prob = 0.5) # Cluster-level categorical variable
b <- b[g]

# Parameters
sd_g <- 0.5           # Standard deviation for the random effect
sigma <- 1            # Standard deviation for observation

# Generate random effects
re0 <- rnorm(n_groups, sd = sd_g)
re <- re0[g]

# Linear predictor
lp <- 0 + 0.5*x + 0.25*b + re

# Continuous and binary target variables
y <- rnorm(N, mean = lp, sd = sigma)
y_bin <- rbinom(N, size = 1, prob = plogis(lp))

# Create a dataframe
d <- tibble(x, b, y, y_bin, g = factor(g))
head(d)
```

**Visualizing the distribution of the simulated data**

Utilizing histograms for continuous variables (x, y, and b) and a bar
plot for the binary variable y_bin,

```{r histograms, echo=FALSE}
# Plotting histograms for continuous variables
par(mfrow = c(2, 2))  # Set up a 2x2 grid for plotting

hist(d$x, main = "Distribution of x", xlab = "x", ylab = "Frequency")
hist(d$y, main = "Distribution of y", xlab = "y", ylab = "Frequency")
hist(d$b, main = "Distribution of b", xlab = "b", ylab = "Frequency")

# Plotting bar plot for binary variable
barplot(table(d$y_bin), main = "Distribution of y_bin", xlab = "y_bin", ylab = "Frequency")

```

### Linear Mixed-Effects Model (LMM)

The Linear Mixed-Effects Model (LMM) is represented by the formula:

$$ Y = X\beta + Zb + \epsilon $$

where:

-   $Y$ represents the vector of observed responses (dependent
    variable).
-   $X$ is the design matrix for fixed effects.
-   $\beta$ denotes the vector of fixed-effect coefficients.
-   $Z$ is the design matrix for random effects.
-   $b$ represents the vector of random-effect coefficients.
-   $\epsilon$ is the vector of residual errors.

LMMs are employed for analyzing data with hierarchical or nested
structures, accommodating both fixed effects (population-level
parameters) and random effects (variation at the group level). These
models are widely used in various disciplines to account for
within-group and between-group variability, particularly in studies
involving clustered or longitudinal data.

**Fitting Linear Mixed-Effects Model in simulated data**

Now let's model a linear mixed-effects model (LMM) using the `lmer`
function from the `lme4` package. The model is applied to a continuous
outcome variable (`y`) with observation-level continuous predictor
(`x`), cluster-level categorical predictor (`b`), and a random intercept
term for the group identifier (`g`). The `system.time` function is
utilized to measure the computational time required for model fitting.
This model is applied to a large dataset consisting of 2 million
observations distributed among 2000 groups, allowing for the analysis of
hierarchical structures within the data.

```{r lmer_simulated, echo=TRUE}
system.time({
  mixed_big = lmer(y ~ x + b + (1|g))
})
summary(mixed_big, cor = FALSE)

```

The result provides information about the computational time required to
fit the linear mixed-effects model using the lme4 package. Here's the
breakdown of the output:

-   **User time:** 5.297 seconds - This represents the amount of CPU
    time spent executing code within the user space. It includes time
    spent executing the code of the current process.

-   **System time:** 0.458 seconds - This indicates the amount of CPU
    time spent executing system calls within the kernel on behalf of the
    process.

-   **Elapsed time:** 5.827 seconds - This is the total wall-clock time
    elapsed from the start to the end of the execution of the code
    chunk. It includes both user and system time as well as any time
    spent waiting for external events or resources.

Overall, the model suggests that the predictor variable
'x' has a significant effect on the outcome variable, with an estimated
coefficient of 0.500046. Additionally, the categorical variable 'b' also
has a significant effect on the outcome variable, with an estimated
coefficient of 0.243823.

### Generalized Linear Mixed-Effects Model (GLMM)

The Generalized Linear Mixed-Effects Model (GLMM) extends the LMM
framework and is expressed by the formula:

$$ g(\mu) = X\beta + Zb $$

where:

-   $g()$ represents the link function relating the mean of the response
    variable $\mu$ to the linear predictor.
-   $X$, $\beta$, $Z$, and $b$ have the same interpretation as in the
    LMM.

GLMMs accommodate non-normally distributed response variables and
different probability distributions, making them suitable for analyzing
binary, count, or categorical outcomes. By incorporating both fixed and
random effects, GLMMs provide flexibility in modeling correlated or
hierarchical data with non-normal response variables, offering
robustness and versatility in various research contexts.

**Fitting Generalized Linear Mixed-Effects Model(GLMM) with glmer in
simulated data**

Next, a generalized linear mixed-effects model (GLMM) is fitted using
the `glmer` function from the `lme4` package. The model is applied to a
binary outcome variable (`y_bin`) with observation-level continuous
predictor (`x`), cluster-level categorical predictor (`b`), and a random
intercept term for the group identifier (`g`). The `family` argument
specifies the binomial distribution for the outcome variable.

```{r glmer_simulated, echo=TRUE}

system.time({
  mixed_big_glmm = glmer(y_bin ~ x + b + (1|g), family = binomial)
})

summary(mixed_big_glmm)
```

The results indicate that the model fitting process for the generalized
linear mixed-effects model (GLMM) took approximately 113.363 seconds,
with user CPU time accounting for 128.560 seconds and system CPU time for
14.392 seconds. The GLMM summary provides information on model fit,
including AIC, BIC, log-likelihood, and deviance. Scaled residuals show
the distribution of residuals, with values ranging from -5.1255 to
4.2161. Random effects are reported for the group identifier 'g',
indicating a variance of 0.2579 and a standard deviation of 0.5079 for
the random intercept. The fixed effects estimates reveal significant
predictors, with 'x' and 'b' both showing highly significant p-values
(\< 2e-16). The correlation of fixed effects indicates a negligible
correlation between 'x' and 'b'.

**Comparison of Linear Mixed-Effects Model (LMM) and Generalized Linear
Mixed-Effects Model (GLMM) for Handling Large Datasets**

In terms of handling large datasets, LMMs are generally more efficient
and computationally faster than GLMMs, especially when dealing with
continuous outcomes. LMMs are suitable for modeling hierarchical data
structures with random effects while maintaining computational
efficiency. On the other hand, GLMMs are preferred when dealing with
binary or categorical outcomes, as they allow for modeling of the
underlying probability distribution directly. However, due to their
computational complexity, GLMMs may be less efficient for large datasets
compared to LMMs.

Therefore, in scenarios where the outcome variable is continuous and
hierarchical data structures need to be accounted for, the LMM approach
is preferable for handling large datasets efficiently.

### Exploring Statistical Models on Multilevel Data: Insights from the "metacognition" Dataset

Now, we will use The "metacognition" dataset from the "sdmar" package to
explore the behavior of `lmer` and `bam` statistical models on
multilevel data structures. This investigation aims to uncover insights
into the performance and suitability of these models for analyzing
hierarchical data encountered in cognitive experiments and tasks.

**Metacognition Dataset Description:**

The dataset comprises observations from a cognitive psychology
experiment, including the following variables:

-   **id:** Participant ID
-   **age:** Age of the participant
-   **sex:** Gender of the participant (female/male)
-   **block:** Block number of the trial
-   **trial:** Trial number within the block
-   **tilt:** Tilt angle of the stimulus (degrees)
-   **contrast:** Contrast level of the stimulus
-   **correct:** Binary indicator of correct response (0 = incorrect, 1
    = correct)
-   **visibility:** Visibility level of the stimulus
-   **confidence:** Confidence level in the response

The dataset contains a total of 7560 observations. Numeric variables
include age, block, trial, tilt, contrast, visibility, confidence, and
correct, while sex is a categorical variable.

Summary statistics for the numeric variables are as follows:\
- **age:** Minimum age is 20, maximum age is 30, with a mean of
approximately 23.85.\
- **tilt:** Tilt angle ranges from 0 to 90 degrees, with a mean of
approximately 44.06 degrees.\
- **contrast:** Contrast level ranges from 0 to 6.9, with a mean of
approximately 3.917.\
- **visibility:** Visibility level ranges from 0 to 100, with a mean of
approximately 39.202.\
- **confidence:** Confidence level ranges from 0 to 100, with a mean of
approximately 58.52.

The majority of participants are female (6048), and the remaining are
male (1512). The data are organized into blocks and trials within each
block, with varying levels of tilt, contrast, visibility, and
confidence. The correct response rate is approximately 77%.

```{r metacognition_dataset, echo=FALSE}
data("metacognition")
head(metacognition)
summary(metacognition)
```

**Checking if any missing values are there in our
`metacognition`dataset**

```{r metacognition_dataset_missing_value_check, echo=FALSE}
# Check for missing values in dataframe df
missing_values <- colSums(is.na(metacognition))

# Display the number of missing values for each column
print(missing_values)

# Optionally, you can visualize the missing value counts
barplot(missing_values, main = "Missing Values Count", xlab = "Columns", ylab = "Missing Values Count")

```

#### Exploring the Influence of Stimulus Visibility on Confidence Levels: Investigating Individual Variations and Gender Differences using `lmer` and `bam` model

**Research Question 1: Model 1**

**"How does the visibility of stimuli influence individuals' confidence
in their responses, considering potential individual variations in
response patterns?"**

```{r lmer_metacognition_rq1, echo=TRUE}
# Fit linear mixed-effects model
system.time({
  mixed_model1 <- lmer(confidence ~ visibility + (1 | id), data = metacognition)
})

# Summary of the model
summary(mixed_model1)

```

The linear mixed-effects model (LMM) revealed a significant effect of
stimulus visibility on individuals' confidence levels (t(7560) =
135.736, p \< 0.001). Each one-unit increase in visibility corresponded
to a 1.015 unit increase in confidence, with an intercept of
approximately 18.719 when visibility is zero. Variability in intercepts
across participants (Var(id) = 157.0, Std.Dev.(id) = 12.53) underscored
individual differences in baseline confidence levels. The model fitting
process took `0.028` seconds to execute.

**Research Question 2: Model 2**

***"Does the visibility of information significantly impact the
confidence levels of participants, and is there a gender difference in
confidence levels when controlling for visibility?"***

```{r lmer_metacognition_rq2, echo=TRUE}

# Fit linear mixed-effects model with participant ID as a random effect and sex as a fixed effect

system.time({
mixed_model2 <- lmer(confidence ~ visibility + sex + (1 | id), data = metacognition)
})
# Summary of the model
summary(mixed_model2)

```

The second model explores the impact of visibility on confidence levels
and investigates gender differences in confidence while controlling for
visibility. Using the `lmer` function, a linear mixed-effects model was
fitted with participant ID as a random effect and sex as a fixed effect.
The analysis, completed in `0.029` seconds, revealed a significant positive
effect of visibility on confidence levels (t(7560) = 135.740, p \<
0.001), indicating a 1.015 unit increase in confidence with each unit
increase in visibility. However, the effect of gender on confidence was
not significant (t(7560) = -1.166), with male participants
showing slightly lower confidence levels compared to females. Model 2
provides insights into both visibility and gender effects on confidence
levels, enhancing our understanding of participant responses.

Despite the inclusion of an additional predictor, the computational
efficiency is not too different from Model 1.

### Bayesian Additive Model (BAM)

The Bayesian Additive Model
("[BAM](https://www.rdocumentation.org/packages/mgcv/versions/1.9-1/topics/bam)")
on `bam`" ) extends the Generalized Additive Model (GAM) framework and
incorporates Bayesian methods for more efficient handling of large
datasets. BAMs are expressed by the formula:

$$ g(\mu) = f_1(x_1) + f_2(x_2) + \ldots + f_p(x_p) + Zb $$

where:

-   $g()$ represents the link function relating the mean of the response
    variable $\mu$ to the linear predictor.
-   $f_i()$ denotes smooth functions of the predictor variables $x_i$.
-   $Z$ is the design matrix for random effects.
-   $b$ represents the vector of random-effect coefficients.

BAMs offer scalability and computational efficiency by employing
adaptive smoothing techniques and Bayesian inference, making them
suitable for analyzing large or complex datasets with non-linear
relationships and various types of response variables.

**Fitting BAM model in metacognition dataset** The `bam` function in R,
part of the `mgcv` package, stands for "Bayesian Additive Models." It
serves as an extension of the `Generalized Additive Models (gam)`
function, offering enhanced efficiency particularly for handling large
datasets. The distinctive feature of bam lies in its capability to
efficiently manage substantial volumes of data, ranging from hundreds of
thousands to millions of observations, through a combination of adaptive
smoothing techniques and Bayesian methodologies.

The following R code chunk measures the computational time required to
fit a Bayesian additive model (`bam`) to the `metacognition` dataset.
The model formula includes terms for the confidence variable
(`confidence`), with smooth terms for `visibility` and `id`, and a
categorical predictor `sex`. The `bs = "cs"` argument specifies a cubic
regression spline basis for smoothing, while `bs = "re"` indicates a
random effect for the `id` variable. The timing results provide insight
into the efficiency of the model fitting process, crucial for assessing
the feasibility of handling large datasets.

```{r bam_metacognition_rq1, echo=TRUE}
# Fit Bayesian additive model
system.time({
bam_model <- bam(confidence ~ s(visibility, bs = "cs") + sex + s(id, bs = "re"), data = metacognition)
})

# Display summary of the model
summary(bam_model)
```

The Bayesian additive model (`bam`) fitting process completed in
approximately `0.023` seconds. The parametric coefficients indicate that
the intercept significantly affects confidence levels (Estimate =
59.609, p \< 0.001), while the effect of `sex` is not statistically
significant (Estimate = -5.454, p = 0.31).

The smooth terms for `visibility` and `id` both exhibit highly
significant effects on confidence levels (p \< 0.001), with effective
degrees of freedom (edf) of 8.275 and 17.835, respectively. The adjusted
R-squared value for this model is 0.807, indicating that approximately
80.8% of the variance in confidence levels is explained.

**Comparing** this `bam` model to mixed_model1 and mixed_model2, the
`bam` model offers a more flexible approach by allowing for non-linear
relationships between predictors and the response variable. Unlike the
linear mixed-effects models used in mixed_model1 and mixed_model2, the
`bam` model can capture more complex patterns in the data. Additionally,
the Bayesian framework of the `bam` model provides advantages in
handling uncertainty and incorporating prior knowledge, which may be
beneficial in certain modeling scenarios.

**Using `BAM` model on metacognition dataset by applying
"[discrete](https://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/bam.html)=TRUE"
in the `bam` function**

In this case a method `discrete` based on discretization of covariate
values and C code level parallelization (controlled by the nthreads
argument instead of the cluster argument) is used. This extends both the
data set and model size that are practical. Number of response data can
not exceed . If discrete is TRUE, a number or a vector of numbers for
each smoother term, then discretization happens. If numbers are supplied
they give the number of discretization bins. Parametric terms use the
maximum number specified. By specifying the $discrete = TRUE$ argument,
the discrete method is invoked during model estimation, enhancing
computational efficiency and scalability, particularly for large
datasets. The summary of the fitted $bam_d_model$ provides insights into
the estimated coefficients, the significance of smooth terms, and model
diagnostics such as adjusted $R^2$ and deviance explained. This approach
is conducive to capturing complex relationships in data while
accommodating the computational demands of large-scale analyses.

```{r bam_metacognition_discrete, echo=TRUE}
# Fit Bayesian additive model with the discrete method
system.time({
bam_d_model <- bam(confidence ~ s(visibility, bs = "cs") + sex + s(id, bs = "re"), data = metacognition, discrete = TRUE)
})
# Display summary of the model
summary(bam_d_model)
``` 

The Bayesian additive model (`bam_d`) with `discrete=TRUE` fitting
process completed in approximately `0.019` seconds, a slightly faster than
the previous `bam` model. In this model, the estimated intercept is
74.563 with a standard error of 2.410, indicating a significant
intercept term ($p < 0.001$). However, the coefficient for the $sex$
predictor ($sexmale$) is estimated to be -5.454 with a standard error of
5.378 and is not statistically significant ($p = 0.31$), suggesting no
significant difference in confidence levels between genders.

Regarding the smooth terms, both $visibility$ and $id$ exhibit
significant and non-significant effects on confidence levels ($p < 0.001$). The smooth term for $visibility$ has an effective degrees of freedom (edf) of 8.235 and
a highly significant F-statistic ($F = 77724.8$), indicating a strong
non-linear relationship between visibility and confidence. Similarly,
the smooth term for $id$ has an edf of 17.835 and a significant
F-statistic ($F = 229.4$), suggesting variability in confidence levels
across different participant IDs.

**Comparing** this model to the previous `bam`model, the coefficients
and significance levels for the $sex$ predictor remain unchanged.
However, the smooth terms for $visibility$ and $id$ show slightly
different results, with a non-significant $visibility$ term in the
current model. This difference may arise from the discrete method used
in fitting the `bam_d_model`, which could affect the estimation of
smooth terms and their significance.

#### Let extract the fixed effect, random effects and variance components of our models(`mixed_model2(lmer)`,`bam_model`,`bam_d_model`)

We utilized the [mixedup package](https://github.com/m-clark/mixedup)
developed by **Michael Clark** to extract fixed effects, random effects,
and variance parameters from mixed models in our analysis.

For clarity and ease of interpretation, within this section of our
notebook, we will use the term `lmer` for `mixed_model2`, `bam` for
`bam_model` and `bam_d` for `bam_d_model`

**Fixed effect, Random effects and Variance components of our `lmer`
model**

```{r fixed_random_variance_mixedmodel2(lmer)_metacognition, echo=TRUE}
mixed_model2_fe = extract_fixed_effects(mixed_model2, digits = 5)
mixed_model2_cov = extract_vc(mixed_model2, digits = 5)
mixed_model2_re = extract_ranef(mixed_model2, digits = 5)

print(mixed_model2_fe)
print(mixed_model2_re)
print(mixed_model2_cov)

```

**Fixed effect, Random effects and Variance components of our `bam`
model**

```{r fixed_random_variance_bam_metacognition_bam, echo=TRUE}
bam_fe   = extract_fixed_effects(bam_model, digits = 5)
bam_vcov = extract_vc(bam_model, digits = 5)
bam_re = extract_ranef(bam_model, digits = 5)

print(bam_fe)
print(bam_re)
print(bam_vcov)
```

**Fixed effect, Random effects and Variance components of our `bam_d`
model**

```{r fixed_random_variance_bam_d_metacognition_bam_discrete, echo=TRUE}
bam_d_fe   = extract_fixed_effects(bam_d_model, digits = 5)
bam_d_vcov = extract_vc(bam_d_model, digits = 5)
bam_d_re = extract_ranef(bam_d_model, digits = 5)

print(bam_d_fe)
print(bam_d_re)
print(bam_d_vcov)
```

**Comparative Analysis of Fixed Effects across Models**

```{r fixed_effects_all_models, echo=FALSE}

# Create data frames for fixed effect results of each model
lmer_fixed_effects <- data.frame(
  term = c("Intercept", "Visibility", "Sex (male)"),
  lmer = c(20.33932, 1.01532, -8.11968),
  bam = c(59.60836, NA, -5.45355),
  bam_d = c(74.56261, NA, -5.45392)
)

# Print the table
kable(lmer_fixed_effects, align = c("l", rep("c", 3)), caption = "Fixed Effects Estimates")


```



**Fixed Effects:** - The `lmer` model estimates the intercept at
approximately 20.34. For the predictor "Sex (male)," the estimate is
consistently around -8.12 across all models. The `lmer` model
additionally incorporates the predictor "Visibility," estimating its
coefficient at approximately 1.02. However, this predictor is absent in
the `BAM` models, where the intercept estimates are notably higher,
around 59.61 and 74.56.

**Comparative Analysis of Random Effects across Models**

```{r random_effects_all_models, echo=FALSE}

# Create data frames for random effect results of each model
random_effects <- data.frame(
  group_var = c("id", "id", "id", "id", "id", "id", "id", "id", "id", "id"),
  effect = rep("Intercept", 10),
  group = 1:10,
  lmer = c(-21.33472, -20.60497, -5.61918, -13.61991, 8.51604, -11.83428, 5.38945, -15.46347, 16.26068, 10.09713),
  bam = c(-19.21317, -8.77917, -6.02752, -10.18568, 2.22848, -4.04100, 1.25456, -12.24857, 12.54605, 6.66087),
  bam_d = c(-19.21731, -8.78148, -6.02769, -10.18517, 2.23672, -4.04305, 1.25200, -12.25231, 12.54553, 6.66161)
)

# Print the table
kable(random_effects, align = c("l", rep("c", 3)), caption = "Random Effects Estimates")

```



**Random Effects:** - Random intercepts for individual participants ("id"
group) exhibit variability across models. In the `lmer` model, these
estimates range from approximately -21.33 to 16.26, while in both `BAM`
models, they range from around -19.22 to 12.55. These individual
deviations from the overall model intercept indicate
participant-specific effects unaccounted for by fixed effects.
Consistency in the range of random effect estimates across models
underscores the substantial contribution of individual participant
characteristics to the observed outcome variability.

**Comparative Analysis of Variance components across Models**

```{r variance_all_models, echo=FALSE}
# Create data frame for variance estimates
variance <- data.frame(
  group = c("id", "Residual"),
  effect = rep("Intercept", 2),
  lmer = c(154.0570, 405.4192),
  bam= c(91.66236, 307.35027),
  bam_d= c(91.67367, 307.35415)
)

# Print the table
kable(variance, align = c("l", rep("c", 3)), caption = "Variance Components Estimates")

```



**Variance:** - Variance estimates for the "id" group in the `lmer`, `BAM`
without `discrete = TRUE`, and `BAM` with `discrete = TRUE` models are
approximately 154.06, 91.66, and 91.67, respectively. These values
represent the variability in outcomes attributed to individual
participant characteristics. Residual variance estimates for all models
are approximately 405.42, 307.35, and 307.35, respectively, indicating
unexplained variability in outcomes. Consistency in variance estimates
for the "id" group across `BAM` models suggests similar capture of
individual variability. These variance components are vital for
understanding individual-level effects and residual variability in
hierarchical data analyses.

**Exploring BAM Model Performance with Varied Parameters: Leveraging
Thread Options and Scaling to Large Dataset's as well as sub sample**

First, we'll assess the performance of Bayesian Additive Models (BAM)
using `subsamples` from the simulated dataset containing 2 million
observations, employing 8 and 10 threads to examine the impact of thread
configuration on model efficiency. Subsequently, we'll extend our
analysis by fitting BAM models to both the entire 2 million dataset and
its subsample, utilizing the discrete = TRUE argument to evaluate the
effectiveness of this method in enhancing model performance, especially
concerning large dataset's. By comparing the results obtained from these
different approaches, we aim to discern any significant differences in
computational efficiency across varying dataset sizes and thread
configurations.

To accomplish this task, a subsample comprising 10,000 observations is
created from the original dataset using the `sample_n()` function,
ensuring reproducibility by setting the seed with `set.seed()`. The sub
sample allows for a manageable dataset size suitable for model testing.

```{r subsample_simulated, echo=TRUE}
# Set seed for reproducibility
set.seed(123)

# Number of observations in the subsample
subsample_size <- 10000

# Create a subsample
subsample <- d %>% 
  sample_n(subsample_size, replace = FALSE)

# Display the first few rows of the subsample
head(subsample)

```

Now, we will investigate the impact of `thread` utilization on the
performance of Bayesian additive models (BAM) fitted to a `subsampled`
dataset. Utilizing a subsampled dataset from a larger population of
observations, we fit two BAM models—one employing 8 threads and the
other utilizing 10 threads. Our objective is to evaluate how variations
in thread allocation influence the computational performance of BAM
models when applied to large datasets. Through this comparative
analysis, we aim to provide insights into the optimal thread utilization
for BAM model fitting in the context of large datasets.

```{r bam_subsample_8thread, echo=TRUE}
system.time({
  bam_big <- bam(
    y_bin ~ x + b + s(g, bs='re'), 
    data = subsample,
    nthreads = 8,
    family = binomial
  )
})
```

```{r bam_subsample_10thread, echo=TRUE}
system.time({
  bam_big <- bam(
    y_bin ~ x + b + s(g, bs='re'), 
    data = subsample,
    nthreads = 10,
    family = binomial
  )
})
```

**Comparing the two BAM models fitted to the subsample:**

-   **Model with 8 Threads:**

    -   Elapsed Time: 812.719 seconds
    
    -   User Time: 800.656 seconds
    
    -   System Time: 4.661 seconds

-   **Model with 10 Threads:**

    -   Elapsed Time: 816.284 seconds

    -   User Time: 807.995 seconds

    -   System Time: 4.238 seconds

Despite increasing the number of threads from 8 to 10, there is no much
significant improvement in the computational efficiency. Both models
have similar elapsed, suggesting that the additional threads did
not lead to a substantial reduction in the time required to fit the BAM.
This indicates that the computational resources available may have been
saturated with 8 threads, resulting in diminishing returns with
additional threads.

**Now, we will utilize the `discrete` parameter in our Bayesian additive
model (BAM) to analyze the 2 million simulated dataset, aiming to assess
the computational time required for model fitting**

```{r bam_simulated_8thread_discrete, echo=TRUE}
system.time({
  bam_big_d <- bam(
    y_bin ~ x + b + s(g, bs='re'), 
    data = d,
    nthreads = 1,
    family = binomial, 
    discrete = TRUE
  )
})
```

Here we can see, the utilization of the BAM model with the `discrete`
parameter enabled revealed a notable reduction in computational time.
This observation underscores the efficiency garnered solely through the
integration of the discrete function, as contrasted with the model
devoid of such functionality.

**Fitting the BAM model on `subsample` of simulated dataset that we
created earlier**

```{r bam_subsample_8thread_discrete_2, echo=TRUE}
system.time({
  bam_big_d <- bam(
    y_bin ~ x + b + s(g, bs='re'), 
    data = subsample,
    nthreads = 1,
    family = binomial, 
    discrete = TRUE
  )
})
```

The computational times for fitting the Bayesian Additive Model (BAM) to both the full dataset and a subsampled dataset of 10,000 observations were compared. Surprisingly, the elapsed time for fitting the model to the subsampled dataset was noticeably longer than that for the full dataset. Specifically, for the full dataset, the user time was 66.210 seconds, the system time was 1.298 seconds, and the elapsed time was 68.379 seconds. In contrast, for the subsampled dataset, the user time increased to 76.527 seconds, with a system time of 1.139 seconds, resulting in an elapsed time of 78.489 seconds. This unexpected discrepancy in computational efficiency between the two datasets warrants further investigation to identify the underlying factors contributing to this difference.

Additionally, a warning arises when using thread counts other than one in macOS, 
indicating the absence of OpenMP and its consequent limitations on parallel computing. 
This underscores the need to account for platform-specific constraints and dataset 
characteristics in statistical modeling analyses.

### Advantages & Limitations of Each Approach

#### Linear Mixed Model (lmer):

**Advantages:** - *Interpretability:* Results are often interpretable,
particularly when the relationship conforms to linearity. - *Wide
Applicability:* Suitable for diverse study designs and various types of
data.

**Disadvantages:** - *Linearity Assumption:* Assumes a linear
relationship between predictors and response variables. - *Sensitivity
to Outliers:* Prone to outlier influence, especially noticeable with
small sample sizes.

#### Generalized Linear Mixed Model (glmer):

**Advantages:** - *Model Flexibility:* Accommodates non-linear
relationships and diverse response distributions. - *Handling Binary
Outcomes:* Well-suited for binary or count-based outcomes.

**Disadvantages:** - *Computational Intensity:* May require substantial
computational resources, particularly for large datasets. - *Model
Complexity:* Increased model complexity may decrease interpretability.

#### Bayesian Additive Model (bam):

**Advantages:** - *Flexibility:* Can effectively capture non-linear
relationships and complex patterns. - *Incorporates Uncertainty:*
Provides estimates along with uncertainty intervals, enhancing result
reliability.

**Disadvantages:** - *Computational Intensity:* Computational demands
can be high, particularly for datasets of considerable size. -
*Expertise Required:* Proper utilization necessitates expertise in
Bayesian statistics.

#### Discrete Bayesian Additive Model (bam_d):

**Advantages:** - *Handling Discrete Predictors:* Beneficial for
analyzing categorical or factor variables. - *Computational Efficiency:*
Offers improved computational efficiency, especially for larger
datasets.

**Disadvantages:** - *Computational Intensity:* While the discrete
option enhances efficiency, it may still demand significant
computational resources. - *Model Complexity:* Introduction of discrete
smoothers may increase model complexity, potentially reducing
interpretability.

### Model Selection Guidelines

When choosing a model for analysis, several key factors should guide
your decision:

1.  **Data Size**: Opt for simpler models with fewer parameters for
    small datasets to avoid overfitting, and consider more complex
    models for larger datasets.

2.  **Computational Resources**: Take into account the computational
    power and time required by different models, especially for
    computationally intensive ones.

3.  **Nature of Relationships**: Select models based on the nature of
    the relationships between variables, whether linear (e.g., lmer) or
    non-linear (e.g., glmer).

4.  **Complexity vs. Interpretability**: Balance the complexity of
    models with their interpretability, prioritizing those that align
    with research goals and allow for meaningful interpretation of
    results.

By considering these factors, researchers can choose the most suitable
model for their data and research questions, ensuring robust statistical
analysis.

### Conclusion

In summary, choosing the right statistical model is crucial for rigorous
analysis when considering Big Data. Linear Mixed Models (lmer) offer
interpretability but rely on linear assumptions and are sensitive to
outliers. Generalized Linear Mixed Models (glmer) accommodate non-linear
relationships but may require significant computational resources.

Bayesian Additive Models (bam) capture complex relationships with
uncertainty estimates, yet demand computational expertise. Discrete
Bayesian Additive Models (bam_d) handle discrete predictors efficiently
but introduce complexity.

Researchers should carefully consider dataset size, computational needs,
and the relationship between variables when selecting a model. Linear
Mixed Models (lmer) suit linear relationships in moderate-sized
datasets, while Generalized Linear Mixed Models (glmer) are preferable
for non-linear relationships or binary outcomes, with attention to
computational demands.

In conclusion, a meticulous alignment of model choice with research
goals and dataset characteristics ensures robust analysis and meaningful
insights in academic pursuits.


### References

Verbeke, G., Molenberghs, G., Fieuws, S., Iddi, S. (2018). Mixed Models
with Emphasis on Large Data Sets. In: Speelman, D., Heylen, K.,
Geeraerts, D. (eds) Mixed-Effects Regression Models in Linguistics.
Quantitative Methods in the Humanities and Social Sciences. Springer,
Cham. [Link](https://doi.org/10.1007/978-3-319-69830-4_2)

Clark (2019, Oct. 20). Michael Clark: Mixed Models for Big Data.
Retrieved from
[Link](https://m-clark.github.io/posts/2019-10-20-big-mixed-models/)

Bates D, Mächler M, Bolker B, Walker S (2015). “Fitting Linear
Mixed-Effects Models Using lme4.” Journal of Statistical Software,
67(1), 1–48. <doi:10.18637/jss.v067.i01>.

Speekenbrink (2023, Nov. 20). Maarten Speekenbrink: Chapter 9 Linear
Mixed Models. Retrieved from
[Link](https://mspeekenbrink.github.io/sdam-r-companion/linear-mixed-effects-models.html)

Wood, Simon N. 2017. Generalized Additive Models: An Introduction with
r, Second Edition. Chapman & Hall/CRC.
[Link](https://doi.org/10.1201/9781315370279)

Rausch, M. & Zehetleitner, M. (2016) Visibility is not equivalent to
confidence in a low contrast orientation discrimination task. Frontiers
in Psychology, 7, p. 591 <doi:10.3389/fpsyg.2016.00591>.

Clark (2019, Oct. 20). Michael Clark: Mixed Models for Big Data.
Retrieved from [Link](https://m-clark.github.io/mixedup/)

Bam R Documentation. Retrieved from
[Link](https://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/bam.html)




### Session info

```{r session_info, echo=FALSE}

sessionInfo()

```
